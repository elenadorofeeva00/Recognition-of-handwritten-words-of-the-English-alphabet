{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Майнор \"Интеллектуальный анализ данных\" </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Курс \"Современные методы машинного обучения\" </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Лабораторная работа №2. Deep Learning. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной лабораторной работе вам предлагается обучить модель на основе нейронной сети для распознавания рукописных букв английского алфавита."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные представлены двумя датасетами: обучающим (`train`) и тестовым (`test`). Изображения для каждого датасета находятся в `images.zip`.  \n",
    "  \n",
    "Обучающая выборка состоит из 65000 изображений - по 2500 изображений для каждой буквы.  \n",
    "Тестовая выборка состоит из 13000 изображений - по 500 изображений для каждой буквы.  \n",
    "  \n",
    "Все изображения - монохромные (но в формате RGB), размерности $28 \\times 28$ пикселей, в формате JPEG. \n",
    "В названии каждого файла содержатся буква, которая представлена на изображении, и уникальный номер изображения: `a_00002.jpg`.  \n",
    "  \n",
    "**NB:** Все изображения представлены в перевернутом виде, для корректного отображения их нужно сначала транспонировать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import *\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import log_loss\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "pic = plt.imread('images/train/a/a_00002.jpg')\n",
    "print(pic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = np.transpose(pic, axes=(1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(pic)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data_dir, data_transforms ):\n",
    "    # create train and test datasets\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                              data_transforms[x])\n",
    "                      for x in ['train', 'test']}\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                                 shuffle=True, num_workers=4)\n",
    "                  for x in ['train', 'test']}\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "    #get classes from train dataset folders name\n",
    "    classes = image_datasets['train'].classes\n",
    "\n",
    "    return dataloaders[\"train\"], dataloaders['test'], classes, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print('Classes: ',  classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datasets have:  {'train': 65000, 'test': 13000}  images\n"
     ]
    }
   ],
   "source": [
    "print('The datasets have: ',  dataset_sizes ,\" images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на наши картиночки из учебной выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5      # unnormalize\n",
    "    #img = np.transpose(img, axes=(1, 0, 2))\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    w     p     s     i\n"
     ]
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "images = [np.transpose(pic, axes=(0, 2, 1)) for pic in images]\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша задача - создать и обучить модель на основе нейронной сети, которая будет предсказывать букву на картинке.  \n",
    "Обучение необходимо проводить на данных из `train`, качество модели проверять на данных из `test`.  \n",
    "Целевая метрика - accuracy.  \n",
    "Для моделирования необходимо использовать `pytorch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1  \n",
    "  \n",
    "*Вес в общей оценке - 0.35*  \n",
    "  \n",
    "1. Постройте и обучите модели с 2-мя и 3-мя полносвязными (dense) скрытыми слоями.  \n",
    "При моделировании необходимо попробовать разные параметры нейронной сети - число нейронов на каждом слое, learning rate, batch size, функции активации, регуляризации и т.д. Оцените качество моделей с различными параметрами, проведите сравнительный анализ. \n",
    "2. Для наилучшей модели постройте confusion matrix результатов предсказаний модели на тестовых данных.  \n",
    "Насколько равномерно обучилась ваша модель? Приведите буквы с самой лучшей и с самой худшей точностью детекции.\n",
    "3. Найдите 10 пар букв, которые чаще всего путаются между собой, дайте возможное объяснение. Приведите примеры с картинками, которые были детектированы с ошибкой.\n",
    "4. Возьмите первую букву вашей фамилии и укажите её точность детекции. С какими буквами ваша модель чаще всего путает эту букву?     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 слоя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подберем batch_size и learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [50, 100, 200, 500, 1000]\n",
    "error = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data_dir, data_transforms, batch_s):\n",
    "    # create train and test datasets\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                              data_transforms[x])\n",
    "                      for x in ['train', 'test']}\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_s,\n",
    "                                                 shuffle=True, num_workers=4)\n",
    "                  for x in ['train', 'test']}\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "    #get classes from train dataset folders name\n",
    "    classes = image_datasets['train'].classes\n",
    "\n",
    "    return dataloaders[\"train\"], dataloaders['test'], classes, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(2352),\n",
    "            nn.Linear(28*28*3, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 26),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()      # (logsoftmax + negative likelihood) in its core, applied to logits\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я буду в первых двух заданиях строить с 2мя эпохами, так как даже 5 эпох приходится очень долго ждать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  50\n",
      "[1,   100] loss: 2.716\n",
      "[1,   200] loss: 1.614\n",
      "[1,   300] loss: 1.221\n",
      "[1,   400] loss: 1.031\n",
      "[1,   500] loss: 0.988\n",
      "[1,   600] loss: 0.885\n",
      "[1,   700] loss: 0.835\n",
      "[1,   800] loss: 0.821\n",
      "[1,   900] loss: 0.790\n",
      "[1,  1000] loss: 0.776\n",
      "[1,  1100] loss: 0.706\n",
      "[1,  1200] loss: 0.682\n",
      "[1,  1300] loss: 0.660\n",
      "[2,   100] loss: 0.613\n",
      "[2,   200] loss: 0.596\n",
      "[2,   300] loss: 0.593\n",
      "[2,   400] loss: 0.560\n",
      "[2,   500] loss: 0.583\n",
      "[2,   600] loss: 0.552\n",
      "[2,   700] loss: 0.565\n",
      "[2,   800] loss: 0.530\n",
      "[2,   900] loss: 0.525\n",
      "[2,  1000] loss: 0.520\n",
      "[2,  1100] loss: 0.503\n",
      "[2,  1200] loss: 0.513\n",
      "[2,  1300] loss: 0.471\n",
      "Finished Training\n",
      "Accuracy: 83 %\n",
      "Batch size:  100\n",
      "[1,   100] loss: 0.426\n",
      "[1,   200] loss: 0.408\n",
      "[1,   300] loss: 0.421\n",
      "[1,   400] loss: 0.408\n",
      "[1,   500] loss: 0.396\n",
      "[1,   600] loss: 0.422\n",
      "[2,   100] loss: 0.371\n",
      "[2,   200] loss: 0.355\n",
      "[2,   300] loss: 0.364\n",
      "[2,   400] loss: 0.356\n",
      "[2,   500] loss: 0.349\n",
      "[2,   600] loss: 0.345\n",
      "Finished Training\n",
      "Accuracy: 86 %\n",
      "Batch size:  200\n",
      "[1,   100] loss: 0.295\n",
      "[1,   200] loss: 0.301\n",
      "[1,   300] loss: 0.306\n",
      "[2,   100] loss: 0.272\n",
      "[2,   200] loss: 0.281\n",
      "[2,   300] loss: 0.273\n",
      "Finished Training\n",
      "Accuracy: 87 %\n",
      "Batch size:  500\n",
      "[1,   100] loss: 0.240\n",
      "[2,   100] loss: 0.231\n",
      "Finished Training\n",
      "Accuracy: 88 %\n",
      "Batch size:  1000\n",
      "Finished Training\n",
      "Accuracy: 88 %\n"
     ]
    }
   ],
   "source": [
    "for batch in batch_size:\n",
    "    trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, batch)\n",
    "    \n",
    "    print('Batch size: ', batch)\n",
    "    \n",
    "    for epoch in range(2): \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "     \n",
    "    print('Finished Training')\n",
    "    \n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc.append(100 * correct / total)\n",
    "            \n",
    "    print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.0001  --- 3 layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2053d586208>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Learning rate = 0.0001  --- 3 layers\")\n",
    "plt.plot(batch_size, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  50\n",
      "[1,   100] loss: 0.390\n",
      "[1,   200] loss: 0.420\n",
      "[1,   300] loss: 0.426\n",
      "[1,   400] loss: 0.447\n",
      "[1,   500] loss: 0.446\n",
      "[1,   600] loss: 0.448\n",
      "[1,   700] loss: 0.454\n",
      "[1,   800] loss: 0.440\n",
      "[1,   900] loss: 0.415\n",
      "[1,  1000] loss: 0.411\n",
      "[1,  1100] loss: 0.423\n",
      "[1,  1200] loss: 0.411\n",
      "[1,  1300] loss: 0.410\n",
      "[2,   100] loss: 0.325\n",
      "[2,   200] loss: 0.336\n",
      "[2,   300] loss: 0.336\n",
      "[2,   400] loss: 0.345\n",
      "[2,   500] loss: 0.330\n",
      "[2,   600] loss: 0.326\n",
      "[2,   700] loss: 0.337\n",
      "[2,   800] loss: 0.359\n",
      "[2,   900] loss: 0.317\n",
      "[2,  1000] loss: 0.337\n",
      "[2,  1100] loss: 0.335\n",
      "[2,  1200] loss: 0.354\n",
      "[2,  1300] loss: 0.341\n",
      "Finished Training\n",
      "Accuracy: 86 %\n",
      "Batch size:  100\n",
      "[1,   100] loss: 0.208\n",
      "[1,   200] loss: 0.197\n",
      "[1,   300] loss: 0.218\n",
      "[1,   400] loss: 0.219\n",
      "[1,   500] loss: 0.236\n",
      "[1,   600] loss: 0.222\n",
      "[2,   100] loss: 0.169\n",
      "[2,   200] loss: 0.178\n",
      "[2,   300] loss: 0.174\n",
      "[2,   400] loss: 0.184\n",
      "[2,   500] loss: 0.186\n",
      "[2,   600] loss: 0.203\n",
      "Finished Training\n",
      "Accuracy: 88 %\n",
      "Batch size:  200\n",
      "[1,   100] loss: 0.111\n",
      "[1,   200] loss: 0.116\n",
      "[1,   300] loss: 0.125\n",
      "[2,   100] loss: 0.099\n",
      "[2,   200] loss: 0.100\n",
      "[2,   300] loss: 0.118\n",
      "Finished Training\n",
      "Accuracy: 89 %\n",
      "Batch size:  500\n",
      "[1,   100] loss: 0.069\n",
      "[2,   100] loss: 0.055\n",
      "Finished Training\n",
      "Accuracy: 90 %\n",
      "Batch size:  1000\n",
      "Finished Training\n",
      "Accuracy: 89 %\n"
     ]
    }
   ],
   "source": [
    "for batch in batch_size:\n",
    "    trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, batch)\n",
    "    \n",
    "    print('Batch size: ', batch)\n",
    "    \n",
    "    for epoch in range(2): \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "     \n",
    "    print('Finished Training')\n",
    "    \n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    error.append(100 * correct / total)\n",
    "            \n",
    "    print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.0005  --- 3 layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2053cc90e48>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Learning rate = 0.0005  --- 3 layers\")\n",
    "plt.plot(batch_size, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для 3х скрытых слоев лучшими значениями оказались learning_rate = 0.0005 и batch_size = 500. Можно объяснить это так: чем мельче кусок данных, на котором мы обучаем модель, тем лучше она обучается. Но важно не переобучится (упасть в локальный минимум?), поэтому learning_rate = 0.0005. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на лучшую нейронную сеть с другими функциями активации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(2352),\n",
    "            nn.Linear(28*28*3, 300),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(300, 200),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(200, 26),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        return x\n",
    "    \n",
    "net2 = Net2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net2.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 3.190\n",
      "[1,    20] loss: 3.034\n",
      "[1,    30] loss: 2.879\n",
      "[1,    40] loss: 2.721\n",
      "[1,    50] loss: 2.563\n",
      "[1,    60] loss: 2.399\n",
      "[2,    10] loss: 2.179\n",
      "[2,    20] loss: 2.046\n",
      "[2,    30] loss: 1.928\n",
      "[2,    40] loss: 1.817\n",
      "[2,    50] loss: 1.712\n",
      "[2,    60] loss: 1.634\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "     \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62 %\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net2(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "            \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net3, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(2352),\n",
    "            nn.Linear(28*28*3, 300),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(300, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 26),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        return x\n",
    "    \n",
    "net3 = Net3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net3.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 2.537\n",
      "[1,    20] loss: 1.838\n",
      "[1,    30] loss: 1.504\n",
      "[1,    40] loss: 1.293\n",
      "[1,    50] loss: 1.176\n",
      "[1,    60] loss: 1.053\n",
      "[2,    10] loss: 0.936\n",
      "[2,    20] loss: 0.894\n",
      "[2,    30] loss: 0.846\n",
      "[2,    40] loss: 0.801\n",
      "[2,    50] loss: 0.787\n",
      "[2,    60] loss: 0.762\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net3(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "     \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78 %\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net3(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "            \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, что если изменим функцию активации в нейронной сети с подобранными параметрами, то accuracy очень сильно снижается, что и логично, так как ReLU и сигмоида с тангенсом имеют абсолютно разные алгоритмы введения нелинейности, что могло повлиять на результат. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на модель с подобранными параметрами с другим числом нейронов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net4, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(2352),\n",
    "            nn.Linear(28*28*3, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 26),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        return x\n",
    "    \n",
    "net4 = Net4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net4.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 2.647\n",
      "[1,    20] loss: 1.480\n",
      "[1,    30] loss: 1.088\n",
      "[1,    40] loss: 0.880\n",
      "[1,    50] loss: 0.797\n",
      "[1,    60] loss: 0.702\n",
      "[2,    10] loss: 0.574\n",
      "[2,    20] loss: 0.566\n",
      "[2,    30] loss: 0.549\n",
      "[2,    40] loss: 0.509\n",
      "[2,    50] loss: 0.491\n",
      "[2,    60] loss: 0.486\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net4(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "     \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84 %\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net4(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "            \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова accuracy стала ниже, но уже не так сильно. Попробуем взять немного другие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net5, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(2352),\n",
    "            nn.Linear(28*28*3, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 26),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        return x\n",
    "    \n",
    "net5 = Net5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net5.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n",
      "Accuracy: 91 %\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net5(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "     \n",
    "print('Finished Training')\n",
    "    \n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net5(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "err.append(100 * correct / total)\n",
    "            \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что accuracy стала значительно лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на модель с подобранными параметрами с функциями регуляризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.0001, 0.0005, 0.005, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight_decay:  0.0001\n",
      "[1,    10] loss: 2.306\n",
      "[1,    20] loss: 1.438\n",
      "[1,    30] loss: 1.086\n",
      "[1,    40] loss: 0.920\n",
      "[1,    50] loss: 0.821\n",
      "[1,    60] loss: 0.739\n",
      "[2,    10] loss: 0.627\n",
      "[2,    20] loss: 0.586\n",
      "[2,    30] loss: 0.550\n",
      "[2,    40] loss: 0.518\n",
      "[2,    50] loss: 0.510\n",
      "[2,    60] loss: 0.492\n",
      "Finished Training\n",
      "Accuracy: 84 %\n",
      "Weight_decay:  0.0005\n",
      "[1,    10] loss: 0.449\n",
      "[1,    20] loss: 0.415\n",
      "[1,    30] loss: 0.442\n",
      "[1,    40] loss: 0.418\n",
      "[1,    50] loss: 0.389\n",
      "[1,    60] loss: 0.399\n",
      "[2,    10] loss: 0.346\n",
      "[2,    20] loss: 0.336\n",
      "[2,    30] loss: 0.345\n",
      "[2,    40] loss: 0.339\n",
      "[2,    50] loss: 0.338\n",
      "[2,    60] loss: 0.333\n",
      "Finished Training\n",
      "Accuracy: 87 %\n",
      "Weight_decay:  0.005\n",
      "[1,    10] loss: 0.307\n",
      "[1,    20] loss: 0.332\n",
      "[1,    30] loss: 0.349\n",
      "[1,    40] loss: 0.348\n",
      "[1,    50] loss: 0.359\n",
      "[1,    60] loss: 0.375\n",
      "[2,    10] loss: 0.341\n",
      "[2,    20] loss: 0.355\n",
      "[2,    30] loss: 0.358\n",
      "[2,    40] loss: 0.367\n",
      "[2,    50] loss: 0.365\n",
      "[2,    60] loss: 0.372\n",
      "Finished Training\n",
      "Accuracy: 87 %\n",
      "Weight_decay:  0.5\n",
      "[1,    10] loss: 0.469\n",
      "[1,    20] loss: 1.025\n",
      "[1,    30] loss: 1.596\n",
      "[1,    40] loss: 1.999\n",
      "[1,    50] loss: 2.287\n",
      "[1,    60] loss: 2.529\n",
      "[2,    10] loss: 2.792\n",
      "[2,    20] loss: 2.926\n",
      "[2,    30] loss: 3.028\n",
      "[2,    40] loss: 3.103\n",
      "[2,    50] loss: 3.161\n",
      "[2,    60] loss: 3.198\n",
      "Finished Training\n",
      "Accuracy: 26 %\n"
     ]
    }
   ],
   "source": [
    "accur = []\n",
    "\n",
    "for weight in weights:\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0005, betas=(0.9, 0.99), weight_decay=weight)\n",
    "    print('Weight_decay: ', weight)\n",
    "    for epoch in range(2): \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                        (epoch + 1, i + 1, running_loss / 10))\n",
    "                running_loss = 0.0\n",
    "     \n",
    "    print('Finished Training')\n",
    "    \n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accur.append(100 * correct / total)\n",
    "            \n",
    "    print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.0005, batch size = 1000  --- 3 layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2053cc93448>]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD7CAYAAABzGc+QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3iUZfr28e+VhBB6b9Kb9B46BAsdBVRQFAUVREUFwr66uqu/1VV/rrovARQRxIKKCqJSLEBATOiQ0HuT3kLvAeR+/2B8l0WEATLzZJLzcxwcU/JM5ryPwZPbZya5zDmHiIiEnjCvA4iIyPVRgYuIhCgVuIhIiFKBi4iEKBW4iEiIUoGLiIQovwrczPqb2UozW2VmA3z35TezeDPb4LvMF9ioIiJysasWuJlVBx4DGgC1gDvMrCLwPDDDOVcRmOG7LSIiQRLhxzFVgPnOuZMAZpYA3AV0Am7xHTMa+AX465W+UcGCBV2ZMmWuM6qISOaUnJy83zlX6NL7/SnwlcDrZlYAOAW0B5KAIs653QDOud1mVvhyDzazPkAfgFKlSpGUlHSdSxARyZzMbOvl7r/qKRTn3BrgTSAemAIsA875+8TOuZHOuWjnXHShQn/4B0RERK6TX29iOuc+dM7Vdc7FAAeBDcBeMysG4LvcF7iYIiJyKX8/hVLYd1kKuBv4EpgE9PQd0hOYGIiAIiJyef6cAwf4xncO/CzwlHPukJn9CxhnZr2AbUDXQIUUEZE/8qvAnXPNL3PfAeD2NE8kIiJ+0U9iioiEKBW4iEiI8vccuKdOn/2NuZv2k3IslQMnzlCxcC5aVimMmXkdTUTEMyFR4F8u3MYrk1f/132tqhbh9c7VKZw7yqNUIiLeColTKIdPngVg1nO3svKVNvy9fRUS16fQclACXydtR3M9RSQzCokCP/PbeSLDwyiZPzs5s0bwWEw5pgyIoXLR3Dw7fjk9P17EzsOnvI4pIhJUIVHgqWfPExnx31HLFszBV30a8c9O1UjacpDWgxL4fP5Wzp/XblxEMofQKPBzv5E14o9Rw8KMHo3LMHVADHVK5ePFCSt5YNR8th444UFKEZHgCokCP3Pujzvwi5XMn53PejXgzXtqsGrnUdoMTmTUrM38pt24iGRgIVHgqefOX3YHfjEz4776pYgf2IKm5Qvy2g9r6PL+XDbuOxaklCIiwRUSBX7m3HmyRoT7dWzRPFGM6hnNkG61+XX/CdoPmc2wmRs599v5AKcUEQmukCjw1HO/XfEUyqXMjE61ixMf24JWVYvw9tR1dH5vDqt3HQ1gShGR4AqJAu/RuAx9byl/zY8rlCsrw7rXZXj3uuw5kkrHd2czKH49Z85pNy4ioc+C+UMw0dHRzquRaodOnOHV71fz7ZKdVCqSi7e61KRWybyeZBERuRZmluyci770/pDYgaeFfDkiGXRfbT56OJojp85y13tzeOPHNZw++5vX0URErkumKfDf3Va5CNMGxnBf/ZKMSNxMuyGzWLTloNexRESuWaYrcIDcUVl44+6ajOndkLO/nefeEfN4edIqTqT6PatZRMRzmbLAf9e0QkGmDoihZ+MyjJ63hTaDE5mzcb/XsURE/JKpCxwgR9YIXu5YjXGPNyYyPIzuoxbwwrfLOXr6rNfRRESuKNMX+O/ql8nPj/2b83iLcoxdtJ3WgxL5ee1er2OJiPwpFfhForKE80K7KnzXtym5s0Xw6CdJDBy7lEMnzngdTUTkD1Tgl1GrZF4mP9OMfrdXZNKyXbSKS+CnFbu9jiUi8l9U4H8ia0Q4A1vdzKSnm1E0TxRPjllM3zHJpBxL9TqaiAigAr+qqjflZkLfpjzXthLTV++jdVwCE5fu1Bg3EfGcXwVuZrFmtsrMVprZl2YWZWZlzWyBmW0ws7FmFhnosF6JCA+j7y0V+LF/M8oUzEH/r5bSe3QSe46c9jqaiGRiVy1wMysO9AOinXPVgXCgG/AmEOecqwgcAnoFMmh6UKFwLsY/0YSX7qjKnE37aTUogbGLtmk3LiKe8PcUSgSQzcwigOzAbuA2YLzv66OBzmkfL/0JDzN6NSvL1AExVCuem79+s4KHPlzI9oMnvY4mIpnMVQvcObcT+DewjQvFfQRIBg47537/2fMdQPHLPd7M+phZkpklpaSkpE3qdKB0gRx80bsRr3WuzpJth2gzOJFP523RUGURCRp/TqHkAzoBZYGbgBxAu8scetnmcs6NdM5FO+eiCxUqdCNZ052wMOPBRqWZNrAF0WXy8z8TV9Ft5Hx+3a+hyiISeP6cQmkJ/OqcS3HOnQW+BZoAeX2nVABKALsClDHdK543G6Mfqc+/u9Zi7Z6jtB2cyAeJGqosIoHlT4FvAxqZWXYzM+B2YDUwE+jiO6YnMDEwEUODmdGlXgmmD2xBzM2FeP3HNdw9fC7r92qosogEhj/nwBdw4c3KxcAK32NGAn8FBprZRqAA8GEAc4aMwrmjGPlQPd65vw7bD56kw9BZvDNjA2c1VFlE0limGanmhQPHU3l58momL9tF1WK5eatLTaoXz+N1LBEJMZl+pJoXCuTMyjv312HEQ/VIOZ5Kp2Fz+PfUdaSe0xg3EblxKvAgaFOtKNNjW3BXneK8O3MjHYbOZsm2Q17HEpEQpwIPkjzZs/DvrrX45JH6nEw9xz3D5/L6D6s5dUa7cRG5PirwILulUmGmxsbwQMNSfDDrV9oNSWT+5gNexxKREKQC90CuqCy81rkGXzzWkPMOuo2cz0sTVnJcQ5VF5BqowD3UpHxBpgxoTq9mZfl8wVbaxCWSuD7j/LoBEQksFbjHskdG8NIdVRn/RBOisoTR46OFPDd+GUdOaaiyiFyZCjydqFc6Hz/0a07fW8rzzeKdtI5LIH61hiqLyJ9TgacjUVnCea5tZSb0bUq+7JE89mkS/b9awkENVRaRy1CBp0M1SuRh0tPNiG15Mz+u2E2rQQn8sHy3BkeIyH9RgadTkRFh9G9ZkcnPNKN4vmw89cVinvg8mX3HNMZNRC5QgadzlYvm5tsnm/B8u8rMXJdCq0GJfJO8Q7txEVGBh4KI8DCeaFGen/o3p2LhnPzl62U88skidh0+5XU0EfGQCjyElC+Uk3GPN+blO6uyYPNBWscl8sUCDVUWyaxU4CEmLMx4uOmFoco1S+Thb9+toPuoBWw7oKHKIpmNCjxElSqQnTG9G/LG3TVYvuMIbQYn8vGcXzVUWSQTUYGHMDPj/galmBYbQ6Ny+Xll8mruHTGPTSnHvY4mIkGgAs8AbsqbjY8ers+ge2uxYd9x2g2ZxfBfNnFOY9xEMjQVeAZhZtxdtwTxA2O4rVJh3pyylrvem8vaPUe9jiYiAaICz2AK54ri/Yfq8V73uuw6fIo735nN4OnrOXNOu3GRjEYFnkG1r1GM+IEt6FCjGIOnb6Dju7NZseOI17FEJA2pwDOw/DkiGdytDqN6RHPo5Bk6vzeHN6es5fRZjXETyQhU4JlAy6pFmBbbgi51SzD8l020HzqL5K0HvY4lIjfoqgVuZpXMbOlFf46a2QAzy29m8Wa2wXeZLxiB5frkyZaFN7vU5LNeDUg9e54u78/jlcmrOHlGY9xEQtVVC9w5t845V9s5VxuoB5wEvgOeB2Y45yoCM3y3JZ1rXrEQU2NjeKhRaT6es4W2g2cxd9N+r2OJyHW41lMotwObnHNbgU7AaN/9o4HOaRlMAidn1gj+2ak6Y/s0IszggQ8W8PfvVnDstMa4iYSSay3wbsCXvutFnHO7AXyXhS/3ADPrY2ZJZpaUkqKBvelJw3IF+Kl/DH1iyvHlwm20iUtk5rp9XscSET+Zv7/JzswigV1ANefcXjM77JzLe9HXDznnrngePDo62iUlJd1QYAmMJdsO8dz45WzYd5x76pbgpTuqkDd7pNexRAQws2TnXPSl91/LDrwdsNg59/uk3b1mVsz3zYsB2rqFsDql8vF9v2Y8c1sFJizdSau4RKau2uN1LBG5gmsp8Pv5z+kTgElAT9/1nsDEtAol3sgaEc5fWldi4lNNKZQzK49/lszTXyzmwPFUr6OJyGX4dQrFzLID24FyzrkjvvsKAOOAUsA2oKtz7oofLtYplNBx9rfzjEjYxNAZG8kZFcHLHatxZ81imJnX0UQynT87heL3OfC0oAIPPev3HuPZ8ctZtv0wraoW4bXO1SmSO8rrWCKZSlqcA5dM6OYiufj2ySa82KEKietTaDkogXFJ2zXGTSQdUIHLVYWHGb2bl2PKgBiqFMvNc+OX0+Ojhew4pDFuIl5SgYvfyhbMwVePNeLVTtVI3nqINnGJfDZ/q8a4iXhEBS7XJCzMeKhxGaYOiKFu6Xy8NGEl938wny37T3gdTSTTUYHLdSmZPzufPtqAt+6pyerdR2k7JJFRszbzm3bjIkGjApfrZmbcW78k8bEtaFahIK/9sIYu789l475jXkcTyRRU4HLDiuaJ4oMe0QzpVpst+0/Qfshshs3cyFkNVRYJKBW4pAkzo1Pt4sQPbEGrqkV4e+o6Og+bw6pdGuMmEigqcElTBXNmZVj3ugzvXpe9R1Pp9O4cBk1bR+o5jXETSWsqcAmIdjWKMX1gDB1r38TQnzdy5zuzWbr9sNexRDIUFbgETN7skQy6tzYfP1yfY6fPcfd7c3jjxzUaqiySRlTgEnC3Vi7M1NgY7qtfihGJm2k3ZBaLtmiossiNUoFLUOSOysIbd9dgTO+GnDt/nntHzOPlSas4kaqhyiLXSwUuQdW0QkGm9I+hZ+MyjJ63hTaDE5m9QUOVRa6HClyCLkfWC79ffNzjjYkMD+PBDxfw/DfLOaqhyiLXRAUunqlfJj8/9m/O4y3KMS5pO60HJfLz2r1Xf6CIACpw8VhUlnBeaFeF7/o2JU+2LDz6SRKxY5dy6MQZr6OJpHsqcEkXapXMy+RnmtH/9opMXraLVnEJ/LRit9exRNI1FbikG5ERYcS2uplJTzejaJ4onhyzmL5jkkk5pqHKIpejApd0p+pNuZnQtynPta3E9DX7aBWXwHdLdmiMm8glVOCSLkWEh9H3lgr82K8Z5QrmIHbsMnqNTmL3kVNeRxNJN1Tgkq5VKJyLr59owkt3VGXupv20HpTIVwu3aTcuggpcQkB4mNGrWVmmDoihWvHcPP/tCh76cCHbD2qosmRuKnAJGaUL5OCL3o14/a7qLN1+mDaDExk9d4uGKkum5VeBm1leMxtvZmvNbI2ZNTaz/GYWb2YbfJf5Ah1WJCzM6N6wNFNjY4guk59/TFrFfSPnsTnluNfRRILO3x34EGCKc64yUAtYAzwPzHDOVQRm+G6LBEXxvNkY/Uh9/t21Fuv2HKPdkFmMTNykocqSqdjV3gwys9zAMqCcu+hgM1sH3OKc221mxYBfnHOVrvS9oqOjXVJSUhrEFvmPfUdP8/cJK4lfvZdaJfPydpea3Fwkl9exRNKMmSU756Ivvd+fHXg5IAX42MyWmNkoM8sBFHHO7QbwXRb+kyfuY2ZJZpaUkpJyA0sQubzCuaMY+VA93rm/DtsPnqTD0FkMnbFBQ5Ulw/OnwCOAusBw51wd4ATXcLrEOTfSORftnIsuVKjQdcYUuTIz485aNxEfG0Pb6sUYFL+eju/OYeVODVWWjMufAt8B7HDOLfDdHs+FQt/rO3WC73JfYCKK+K9Azqy8c38dRj5UjwPHU+k0bA5vT12rMW6SIV21wJ1ze4DtZvb7+e3bgdXAJKCn776ewMSAJBS5Dq2rFSU+tgV31ynOsJmbuOOd2SzedsjrWCJp6qpvYgKYWW1gFBAJbAYe4UL5jwNKAduArs65Kw461JuY4oWE9Sm88M1ydh89Ta+mZflL60pkiwz3OpaI3/7sTUy/CjytqMDFK8dOn+XNKWv5fP42ShfIzpv31KRRuQJexxLxy418CkUk5OWKysJrnWvw5WONcA66jZzPixNWcFxDlSWEqcAlU2lcvgBTBjSnV7OyjFmwjTZxiSSu18dbJTSpwCXTyR4ZwUt3VGX8E02IyhJGj48W8uzXyzhyUkOVJbSowCXTqlc6Hz/0a85Tt5bn2yU7aRWXQPxqDVWW0KECl0wtKks4z7apzMSnmpI/RySPfZpEvy+XcFBDlSUEqMBFgOrF8zDp6WYMbHUzP63cTatBCXy/fJcGR0i6pgIX8YmMCKPf7RX5/pnmlMiXjae/WMITnyez7+hpr6OJXJYKXOQSlYrm4psnm/BCu8rMXJdCq7hEvknWUGVJf1TgIpcRER7G4y3K81P/5lQsnJO/fL2MRz5ZxK7DGqos6YcKXOQKyhfKybjHG/NKx2os/PUgreMSGbNgq8a4SbqgAhe5irAwo2eTMkwdEEPNEnn4+3cr6T5qAdsOaKiyeEsFLuKnkvmzM6Z3Q964uwYrdx6hzeBEPpr9q8a4iWdU4CLXwMy4v0Eppg2MoVG5/Pzz+9XcO2IeG/dpqLIEnwpc5DoUy5ONjx6uT9x9tdi47zjth85i+C+bOKcxbhJEKnCR62Rm3FWnBPEDY7itUmHenLKWu96by9o9R72OJpmEClzkBhXOFcX7D9Xjve512X3kFHe+M5u4+PWcOafduASWClwkjbSvUYxpsS3oUKMYQ2ZsoOO7s1m+47DXsSQDU4GLpKH8OSIZ3K0Oo3pEc+jkGToPm8O/ftJQZQkMFbhIALSsWoRpsS3oWq8k7ydsov3QWSRtueLIWJFrpgIXCZA82bLwZpeafNarAalnz9N1xDxembyKk2c0xk3ShgpcJMCaVyzEtNgYejQqzcdzttBmcCJzN+73OpZkACpwkSDIkTWCVzpVZ2yfRoSb8cCoBbzw7QqOntYYN7l+KnCRIGpYrgA/9Y+hT0w5xi66MFR55rp9XseSEOVXgZvZFjNbYWZLzSzJd19+M4s3sw2+y3yBjSqSMWSLDOdv7avwzZNNyJk1gkc+XsTAcUs5fFJj3OTaXMsO/FbnXG3nXLTv9vPADOdcRWCG77aI+KlOqXx8368Zz9xWgUlLd9FyUCJTVu7xOpaEkBs5hdIJGO27PhrofONxRDKXrBHh/KV1JSY+3ZQiubPyxOfJPPXFYvYfT/U6moQAfwvcAdPMLNnM+vjuK+Kc2w3guyx8uQeaWR8zSzKzpJSUlBtPLJIBVbspDxOeasqzbSoRv2ovrQYlMHHpTo1xkysyf/6CmNlNzrldZlYYiAeeASY55/JedMwh59wVz4NHR0e7pKSkG80skqFt2HuMZ8cvZ+n2w7SsUoTX76pOkdxRXscSD5lZ8kWnr/8/v3bgzrldvst9wHdAA2CvmRXzffNigN5KF0kDFYtcGKr8YocqzNqQQstBCYxbtF27cfmDqxa4meUws1y/XwdaAyuBSUBP32E9gYmBCimS2YSHGb2bl2PKgBiqFMvNc98sp8dHC9lxSGPc5D/82YEXAWab2TJgIfCDc24K8C+glZltAFr5botIGipbMAdfPdaIVztVI3nrIdrEJfLZvC0aqiyAn+fA04rOgYtcv+0HT/K371Ywa8N+GpTNz1v31KRMwRxex5IguKFz4CLivZL5s/Ppow14q0tN1uw+StshiYyatVlDlTMxFbhICDEz7o0uyfSBLWhWoSCv/bCGe4bPZcPeY15HEw+owEVCUJHcUXzQI5oh3Wqz9cAJOgydzbCZGzmrocqZigpcJESZGZ1qFyd+YAtaVSvC21PX0endOazadcTraBIkKnCREFcwZ1aGPVCX9x+sy75jqXR6dw6Dpq0j9ZzGuGV0KnCRDKJt9WJMHxhDx9o3MfTnjdwxdDZLth3yOpYEkApcJAPJmz2SQffW5uNH6nM89Rz3DJ/L//64RkOVMygVuEgGdGulwkyLjaFbg1KMTNxMuyGzWPirhipnNCpwkQwqV1QW/veuGozp3ZBz589z74h5/GPiSk6kaqhyRqECF8ngmlYoyNQBMTzStAyfzt9Km8GJzN6gocoZgQpcJBPIHhnBP+6sxtePNyYyPIwHP1zA898s11DlEKcCF8lEosvk58f+zXmiRXnGJW2n9aBEZqzZ63UsuU4qcJFMJipLOM+3q8x3fZuSJ1sWeo1OYsBXSzh0QkOVQ40KXCSTqlUyL5OfaUb/2yvy/fLdtIpL4McVu72OJddABS6SiUVGhBHb6mYmP9OMonmi6DtmMU9+nkzKMQ1VDgUqcBGhSrHcTOjblOfaVmLG2n20ikvguyU7NMYtnVOBiwgAEeFh9L2lAj/2a065gjmIHbuMXqOT2H3klNfR5E+owEXkv1QonJOvn2jC/9xRlbmb9tN6UCJfLdym3Xg6pAIXkT8IDzMebVaWqQNiqF48D89/u4IHP1zA9oMaqpyeqMBF5E+VLpCDMb0b8vpd1Vm2/Qit4xL5ZM6vGqqcTqjAReSKwsKM7g1LMzU2hgZl8/Py5NXcN3Iem1OOex0t01OBi4hfiufNxieP1OffXWuxbs8x2g2ZxYiETZzTGDfPqMBFxG9mRpd6JZg+sAUtbi7EGz+t5Z7hc1m3R0OVveB3gZtZuJktMbPvfbfLmtkCM9tgZmPNLDJwMUUkPSmcO4oRD9Xj3QfqsP3QKe54ZxZDZ2zQUOUgu5YdeH9gzUW33wTinHMVgUNAr7QMJiLpm5lxR82biI+NoV31YgyKX0/Hd+ewcqeGKgeLXwVuZiWADsAo320DbgPG+w4ZDXQOREARSd8K5MzK0PvrMPKhehw4nkqnYXN4a8pajXELAn934IOB54Df//+oAHDYOff7aI8dQPE0ziYiIaR1taLEx7bg7jrFee+XTXQYOovkrRqqHEhXLXAzuwPY55xLvvjuyxx62Q+GmlkfM0sys6SUlJTrjCkioSBP9iy83bUWox9twKkzv9Hl/bm8+v1qTp3RbjwQ/NmBNwU6mtkW4CsunDoZDOQ1swjfMSWAXZd7sHNupHMu2jkXXahQoTSILCLpXYubCzE1NobuDUvx4exfaTskkXmbDngdK8O5aoE7515wzpVwzpUBugE/O+e6AzOBLr7DegITA5ZSREJOrqgsvNa5Bl/1aQTA/R/M58UJKziuocpp5kY+B/5XYKCZbeTCOfEP0yaSiGQkjcoVYEr/GHo3K8uYBdtoE5dIwnqdTk0LFszfMBYdHe2SkpKC9nwikr4kbz3Ec+OXsSnlBF3rleDFDlXJkz2L17HSPTNLds5FX3q/fhJTRIKmXul8/NCvOU/dWp5vl+ykZVwC01bt8TpWyFKBi0hQRWUJ59k2lZn4VFMK5Iikz2fJPPPlEg4c1xi3a6UCFxFPVC+eh0lPN2Ngq5uZsnI3reISmbxslwZHXAMVuIh4JjIijH63V+T7Z5pTMl82nvlyCY9/lsy+o6e9jhYSVOAi4rlKRXPxzZNN+Fv7yiSsT6HloATGJ2uo8tWowEUkXYgID6NPTHl+6t+cSkVz8X++XsbDHy9i52ENVf4zKnARSVfKFcrJ2D6NeaVjNRZtOUjrQQl8Pn+rxrhdhgpcRNKdsDCjZ5MyTB0QQ+1SeXlxwkq6j1rA1gMnvI6WrqjARSTdKpk/O5/3asi/7q7Byp1HaDM4kQ9n/8pv2o0DKnARSefMjG4NSjFtYAxNyhfk1e9X0/X9uWzcp6HKKnARCQnF8mTjw57RDL6vNpv3n6D90Fm898vGTD1UWQUuIiHDzOhcpzjxsS24vXJh3pqyjrvem8ua3Ue9juYJFbiIhJxCubIy/MF6vNe9LruPnOLOd2YTF7+eM+cy125cBS4iIat9jWLEx7bgzlo3MWTGBu58ZzbLdxz2OlbQqMBFJKTlyxFJ3H21+bBnNIdPnaHzsDn866fMMVRZBS4iGcLtVYowLbYF90aX5P2ETbQfMoukLQe9jhVQKnARyTDyZMvCv+6pyWe9GpB67jxdR8zj5UmrOHkmY45xU4GLSIbTvGIhpsXG0KNRaT6Zu4U2gxOZu3G/17HSnApcRDKkHFkjeKVTdcY93phwMx4YtYAXvl3B0dNnvY6WZlTgIpKhNSibnykDYng8phxjF10Yqjxz7T6vY6UJFbiIZHhRWcJ5oX0Vvu3blFxRETzyySIGjlvK4ZNnvI52Q1TgIpJp1C6Zl8nPNKPfbRWYtHQXLQclMmXlbq9jXTcVuIhkKlkjwhnYuhITn25KkdxZeeLzxTw1ZjH7Q3CosgpcRDKlajflYcJTTXm2TSXiV++l1aAEJi7dGVJj3K5a4GYWZWYLzWyZma0ys1d895c1swVmtsHMxppZZODjioiknSzhYTx1awV+6NeM0gVy0P+rpTz2aRJ7joTGUGV/duCpwG3OuVpAbaCtmTUC3gTinHMVgUNAr8DFFBEJnIpFLgxVfrFDFWZt2E+ruATGLdqe7nfjVy1wd8Hvvzk9i++PA24DxvvuHw10DkhCEZEgCA8zejcvx9QBMVQtlpvnvllOj48WsuPQSa+j/Sm/zoGbWbiZLQX2AfHAJuCwc+73n0/dART/k8f2MbMkM0tKSUlJi8wiIgFTpmAOvnysEa92rs7irYdoE5fIZ/O2pMuhyn4VuHPuN+dcbaAE0ACocrnD/uSxI51z0c656EKFCl1/UhGRIAkLMx5qVJqpsTHULZ2PlyauotsH8/l1f/oaqnxNn0Jxzh0GfgEaAXnNLML3pRLArrSNJiLirRL5svPpow14q0tN1uw+StvBiXyQuDndDFX251Mohcwsr+96NqAlsAaYCXTxHdYTmBiokCIiXjEz7o0uyfSBLWhesSCv/7iGe4bPZcPeY15H82sHXgyYaWbLgUVAvHPue+CvwEAz2wgUAD4MXEwREW8VyR3FBz2iGdKtNlsPnKDD0Nm8+/MGzno4VNmC+TGZ6Ohol5SUFLTnExEJhP3HU/nHpFX8sHw3VYvl5u2uNal2U56APZ+ZJTvnoi+9Xz+JKSJyjQrmzMqwB+ry/oP12HcslU7vzuH/TltH6rngjnFTgYuIXKe21YsyfWAMnWoX552fN3LH0Nks2XYoaM+vAhcRuQF5s0fyf++txceP1Od46jnuGT6X139Yzakzgd+Nq8BFRNLArZUKMy02hm4NSvHBrF9pNySRBZsPBPQ5VeAiImkkV1QW/veuGnzRuyG/Ocd9I+fzPxNXcjw1MEOVVWKsxt8AAAP+SURBVOAiImmsSYWCTB0QwyNNy/DZ/K20iUtk3Z60/9y4ClxEJACyR0bwjzur8fXjjSlfOCcl8mVL8+eIuPohIiJyvaLL5OfTRxsE5HtrBy4iEqJU4CIiIUoFLiISolTgIiIhSgUuIhKiVOAiIiFKBS4iEqJU4CIiISqoAx3MLAXYep0PLwjsT8M4oUBrzhy05ozvRtdb2jn3h6nwQS3wG2FmSZebSJGRac2Zg9ac8QVqvTqFIiISolTgIiIhKpQKfKTXATygNWcOWnPGF5D1hsw5cBER+W+htAMXEZGLqMBFREJUuitwM2trZuvMbKOZPX+Zr2c1s7G+ry8wszLBT5m2/FhzjJktNrNzZtbFi4xpyY/1DjSz1Wa23MxmmFlpL3KmJT/W/ISZrTCzpWY228yqepEzLV1tzRcd18XMnJmF/McK/XidHzazFN/rvNTMet/QEzrn0s0fIBzYBJQDIoFlQNVLjukLvO+73g0Y63XuIKy5DFAT+BTo4nXmIKz3ViC77/qTmeQ1zn3R9Y7AFK9zB3rNvuNyAYnAfCDa69xBeJ0fBt5Nq+dMbzvwBsBG59xm59wZ4Cug0yXHdAJG+66PB243MwtixrR21TU757Y455YD570ImMb8We9M59xJ3835QIkgZ0xr/qz56EU3cwCh/ukCf/5bBngVeAs4HcxwAeLvmtNMeivw4sD2i27v8N132WOcc+eAI0CBoKQLDH/WnJFc63p7AT8FNFHg+bVmM3vKzDZxodD6BSlboFx1zWZWByjpnPs+mMECyN+/2/f4Tg+ON7OSN/KE6a3AL7eTvnQn4s8xoSSjredq/F6vmT0IRANvBzRR4Pm1ZufcMOdceeCvwIsBTxVYV1yzmYUBccBfgpYo8Px5nScDZZxzNYHp/OdswnVJbwW+A7j4X6QSwK4/O8bMIoA8wMGgpAsMf9ackfi1XjNrCfwd6OicSw1StkC51tf4K6BzQBMF3tXWnAuoDvxiZluARsCkEH8j86qvs3PuwEV/nz8A6t3IE6a3Al8EVDSzsmYWyYU3KSddcswkoKfvehfgZ+d7dyBE+bPmjOSq6/X9r/UILpT3Pg8ypjV/1lzxopsdgA1BzBcIV1yzc+6Ic66gc66Mc64MF97r6OicS/Imbprw53UudtHNjsCaG3pGr9+5vcw7ue2B9Vx4N/fvvvv+yYUXFyAK+BrYCCwEynmdOQhrrs+Ff91PAAeAVV5nDvB6pwN7gaW+P5O8zhyENQ8BVvnWOxOo5nXmQK/5kmN/IcQ/heLn6/yG73Ve5nudK9/I8+lH6UVEQlR6O4UiIiJ+UoGLiIQoFbiISIhSgYuIhCgVuIhIiFKBi4iEKBW4iEiI+n8xSkBqOJVNiwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Learning rate = 0.0005, batch size = 1000  --- 3 layers\")\n",
    "plt.plot(weights, accur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(2352),\n",
    "            nn.Linear(28*28*3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 26),\n",
    "            nn.ReLU(),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        return x\n",
    "    \n",
    "net_ = Net_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "optimizer = torch.optim.Adam(net_.parameters(), lr=0.0001, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  50\n",
      "[1,   100] loss: 2.642\n",
      "[1,   200] loss: 1.799\n",
      "[1,   300] loss: 1.406\n",
      "[1,   400] loss: 1.224\n",
      "[1,   500] loss: 1.145\n",
      "[1,   600] loss: 1.059\n",
      "[1,   700] loss: 1.008\n",
      "[1,   800] loss: 0.986\n",
      "[1,   900] loss: 0.954\n",
      "[1,  1000] loss: 0.879\n",
      "[1,  1100] loss: 0.909\n",
      "[1,  1200] loss: 0.853\n",
      "[1,  1300] loss: 0.847\n",
      "[2,   100] loss: 0.816\n",
      "[2,   200] loss: 0.776\n",
      "[2,   300] loss: 0.747\n",
      "[2,   400] loss: 0.751\n",
      "[2,   500] loss: 0.703\n",
      "[2,   600] loss: 0.741\n",
      "[2,   700] loss: 0.739\n",
      "[2,   800] loss: 0.714\n",
      "[2,   900] loss: 0.703\n",
      "[2,  1000] loss: 0.690\n",
      "[2,  1100] loss: 0.709\n",
      "[2,  1200] loss: 0.657\n",
      "[2,  1300] loss: 0.653\n",
      "Finished Training\n",
      "Accuracy: 79 %\n",
      "Batch size:  100\n",
      "[1,   100] loss: 0.622\n",
      "[1,   200] loss: 0.589\n",
      "[1,   300] loss: 0.587\n",
      "[1,   400] loss: 0.597\n",
      "[1,   500] loss: 0.575\n",
      "[1,   600] loss: 0.579\n",
      "[2,   100] loss: 0.551\n",
      "[2,   200] loss: 0.524\n",
      "[2,   300] loss: 0.554\n",
      "[2,   400] loss: 0.539\n",
      "[2,   500] loss: 0.519\n",
      "[2,   600] loss: 0.517\n",
      "Finished Training\n",
      "Accuracy: 82 %\n",
      "Batch size:  200\n",
      "[1,   100] loss: 0.474\n",
      "[1,   200] loss: 0.480\n",
      "[1,   300] loss: 0.482\n",
      "[2,   100] loss: 0.441\n",
      "[2,   200] loss: 0.466\n",
      "[2,   300] loss: 0.463\n",
      "Finished Training\n",
      "Accuracy: 83 %\n",
      "Batch size:  500\n",
      "[1,   100] loss: 0.423\n",
      "[2,   100] loss: 0.411\n",
      "Finished Training\n",
      "Accuracy: 83 %\n",
      "Batch size:  1000\n",
      "Finished Training\n",
      "Accuracy: 83 %\n"
     ]
    }
   ],
   "source": [
    "for batch in batch_size:\n",
    "    trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, batch)\n",
    "    \n",
    "    print('Batch size: ', batch)\n",
    "    \n",
    "    for epoch in range(2): \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net_(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "     \n",
    "    print('Finished Training')\n",
    "    \n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net_(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc_.append(100 * correct / total)\n",
    "            \n",
    "    print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.0001  --- 2 layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2053c754208>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Learning rate = 0.0001  --- 2 layers\")\n",
    "plt.plot(batch_size, acc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net_.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  50\n",
      "[1,   100] loss: 0.516\n",
      "[1,   200] loss: 0.584\n",
      "[1,   300] loss: 0.544\n",
      "[1,   400] loss: 0.599\n",
      "[1,   500] loss: 0.589\n",
      "[1,   600] loss: 0.595\n",
      "[1,   700] loss: 0.568\n",
      "[1,   800] loss: 0.543\n",
      "[1,   900] loss: 0.572\n",
      "[1,  1000] loss: 0.541\n",
      "[1,  1100] loss: 0.568\n",
      "[1,  1200] loss: 0.540\n",
      "[1,  1300] loss: 0.540\n",
      "[2,   100] loss: 0.435\n",
      "[2,   200] loss: 0.475\n",
      "[2,   300] loss: 0.446\n",
      "[2,   400] loss: 0.481\n",
      "[2,   500] loss: 0.510\n",
      "[2,   600] loss: 0.488\n",
      "[2,   700] loss: 0.510\n",
      "[2,   800] loss: 0.483\n",
      "[2,   900] loss: 0.498\n",
      "[2,  1000] loss: 0.487\n",
      "[2,  1100] loss: 0.500\n",
      "[2,  1200] loss: 0.502\n",
      "[2,  1300] loss: 0.520\n",
      "Finished Training\n",
      "Accuracy: 82 %\n",
      "Batch size:  100\n",
      "[1,   100] loss: 0.374\n",
      "[1,   200] loss: 0.365\n",
      "[1,   300] loss: 0.362\n",
      "[1,   400] loss: 0.367\n",
      "[1,   500] loss: 0.384\n",
      "[1,   600] loss: 0.371\n",
      "[2,   100] loss: 0.318\n",
      "[2,   200] loss: 0.330\n",
      "[2,   300] loss: 0.327\n",
      "[2,   400] loss: 0.359\n",
      "[2,   500] loss: 0.365\n",
      "[2,   600] loss: 0.351\n",
      "Finished Training\n",
      "Accuracy: 83 %\n",
      "Batch size:  200\n",
      "[1,   100] loss: 0.272\n",
      "[1,   200] loss: 0.275\n",
      "[1,   300] loss: 0.285\n",
      "[2,   100] loss: 0.257\n",
      "[2,   200] loss: 0.274\n",
      "[2,   300] loss: 0.265\n",
      "Finished Training\n",
      "Accuracy: 84 %\n",
      "Batch size:  500\n",
      "[1,   100] loss: 0.231\n",
      "[2,   100] loss: 0.220\n",
      "Finished Training\n",
      "Accuracy: 85 %\n",
      "Batch size:  1000\n",
      "Finished Training\n",
      "Accuracy: 85 %\n"
     ]
    }
   ],
   "source": [
    "for batch in batch_size:\n",
    "    trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, batch)\n",
    "    \n",
    "    print('Batch size: ', batch)\n",
    "    \n",
    "    for epoch in range(2): \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net_(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "     \n",
    "    print('Finished Training')\n",
    "    \n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net_(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    error_.append(100 * correct / total)\n",
    "            \n",
    "    print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.0005  --- 2 layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2053f22ea48>]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Learning rate = 0.0005  --- 2 layers\")\n",
    "plt.plot(batch_size, error_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат снова при batch_size = 1000 и learning_rate = 0.0005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на лучшую нейронную сеть с другими функциями активации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2_(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2_, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(2352),\n",
    "            nn.Linear(28*28*3, 256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(256, 26),\n",
    "            nn.Sigmoid(),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        return x\n",
    "    \n",
    "net2_ = Net2_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net2_.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 3.155\n",
      "[1,    20] loss: 3.017\n",
      "[1,    30] loss: 2.944\n",
      "[1,    40] loss: 2.896\n",
      "[1,    50] loss: 2.851\n",
      "[1,    60] loss: 2.817\n",
      "[2,    10] loss: 2.771\n",
      "[2,    20] loss: 2.748\n",
      "[2,    30] loss: 2.731\n",
      "[2,    40] loss: 2.717\n",
      "[2,    50] loss: 2.702\n",
      "[2,    60] loss: 2.685\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net2_(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "     \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67 %\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net2_(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "            \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net3_(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net3_, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(2352),\n",
    "            nn.Linear(28*28*3, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 26),\n",
    "            nn.Tanh(),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        return x\n",
    "    \n",
    "net3_ = Net3_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net3_.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 2.708\n",
      "[1,    20] loss: 2.497\n",
      "[1,    30] loss: 2.443\n",
      "[1,    40] loss: 2.401\n",
      "[1,    50] loss: 2.343\n",
      "[1,    60] loss: 2.283\n",
      "[2,    10] loss: 2.170\n",
      "[2,    20] loss: 2.110\n",
      "[2,    30] loss: 2.061\n",
      "[2,    40] loss: 2.018\n",
      "[2,    50] loss: 1.983\n",
      "[2,    60] loss: 1.961\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net3_(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "     \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72 %\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net3_(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "            \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на модель с подобранными параметрами с другим числом нейронов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net4_(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net4_, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(2352),\n",
    "            nn.Linear(28*28*3, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 26),\n",
    "            nn.ReLU(),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        return x\n",
    "    \n",
    "net4_ = Net4_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net4_.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 2.324\n",
      "[1,    20] loss: 1.309\n",
      "[1,    30] loss: 1.002\n",
      "[1,    40] loss: 0.877\n",
      "[1,    50] loss: 0.808\n",
      "[1,    60] loss: 0.727\n",
      "[2,    10] loss: 0.617\n",
      "[2,    20] loss: 0.591\n",
      "[2,    30] loss: 0.564\n",
      "[2,    40] loss: 0.517\n",
      "[2,    50] loss: 0.513\n",
      "[2,    60] loss: 0.488\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net4_(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "     \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84 %\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net4_(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "            \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net5_(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net5_, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(2352),\n",
    "            nn.Linear(28*28*3, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 26),\n",
    "            nn.ReLU(),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        return x\n",
    "    \n",
    "net5_ = Net5_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net5_.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n",
      "Accuracy: 82 %\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net5_(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "     \n",
    "print('Finished Training')\n",
    "    \n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net5_(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "err.append(100 * correct / total)\n",
    "            \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на модель с подобранными параметрами с функциями регуляризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight_decay:  0.0001\n",
      "[1,    10] loss: 0.190\n",
      "[1,    20] loss: 0.210\n",
      "[1,    30] loss: 0.204\n",
      "[1,    40] loss: 0.199\n",
      "[1,    50] loss: 0.207\n",
      "[1,    60] loss: 0.208\n",
      "[2,    10] loss: 0.196\n",
      "[2,    20] loss: 0.194\n",
      "[2,    30] loss: 0.197\n",
      "[2,    40] loss: 0.200\n",
      "[2,    50] loss: 0.194\n",
      "[2,    60] loss: 0.203\n",
      "Finished Training\n",
      "Accuracy: 85 %\n",
      "Weight_decay:  0.0005\n",
      "[1,    10] loss: 0.189\n",
      "[1,    20] loss: 0.203\n",
      "[1,    30] loss: 0.206\n",
      "[1,    40] loss: 0.205\n",
      "[1,    50] loss: 0.217\n",
      "[1,    60] loss: 0.201\n",
      "[2,    10] loss: 0.198\n",
      "[2,    20] loss: 0.208\n",
      "[2,    30] loss: 0.194\n",
      "[2,    40] loss: 0.211\n",
      "[2,    50] loss: 0.199\n",
      "[2,    60] loss: 0.227\n",
      "Finished Training\n",
      "Accuracy: 85 %\n",
      "Weight_decay:  0.005\n",
      "[1,    10] loss: 0.177\n",
      "[1,    20] loss: 0.126\n",
      "[1,    30] loss: 0.125\n",
      "[1,    40] loss: 0.143\n",
      "[1,    50] loss: 0.158\n",
      "[1,    60] loss: 0.169\n",
      "[2,    10] loss: 0.176\n",
      "[2,    20] loss: 0.189\n",
      "[2,    30] loss: 0.205\n",
      "[2,    40] loss: 0.211\n",
      "[2,    50] loss: 0.242\n",
      "[2,    60] loss: 0.247\n",
      "Finished Training\n",
      "Accuracy: 88 %\n",
      "Weight_decay:  0.5\n",
      "[1,    10] loss: 0.280\n",
      "[1,    20] loss: 0.508\n",
      "[1,    30] loss: 0.799\n",
      "[1,    40] loss: 1.014\n",
      "[1,    50] loss: 1.179\n",
      "[1,    60] loss: 1.325\n",
      "[2,    10] loss: 1.510\n",
      "[2,    20] loss: 1.606\n",
      "[2,    30] loss: 1.706\n",
      "[2,    40] loss: 1.796\n",
      "[2,    50] loss: 1.884\n",
      "[2,    60] loss: 1.953\n",
      "Finished Training\n",
      "Accuracy: 66 %\n"
     ]
    }
   ],
   "source": [
    "accur_ = []\n",
    "\n",
    "for weight in weights:\n",
    "    optimizer = torch.optim.Adam(net_.parameters(), lr=0.0005, betas=(0.9, 0.99), weight_decay=weight)\n",
    "    print('Weight_decay: ', weight)\n",
    "    for epoch in range(2): \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net_(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                        (epoch + 1, i + 1, running_loss / 10))\n",
    "                running_loss = 0.0\n",
    "     \n",
    "    print('Finished Training')\n",
    "    \n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net_(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accur_.append(100 * correct / total)\n",
    "            \n",
    "    print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.0005, batch size = 1000  --- 2 layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2053ab01b08>]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Learning rate = 0.0005, batch size = 1000  --- 2 layers\")\n",
    "plt.plot(weights, accur_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшей моделью оказалась модель с 3мя скрытыми слоями Net5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "all_pred = []\n",
    "all_images = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net5(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        all_labels = all_labels + list(labels)\n",
    "        all_pred = all_pred + list(predicted)\n",
    "        for im in images:\n",
    "            all_images.append(list(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:  [[  3 462   0   6   3   1  11   2   2   0   0   1   0   0   2   0   2   1\n",
      "    1   1   0   0   0   0   0]\n",
      " [  1   1 476   0   9   0   3   0   0   0   0   3   0   0   3   0   0   1\n",
      "    2   0   1   0   0   0   0]\n",
      " [  3   4   0 463   0   0   0   2   0   3   1   2   0   1  14   1   2   0\n",
      "    1   0   1   0   1   0   0]\n",
      " [  0   2   7   0 476   1   4   0   1   0   0   0   0   0   0   0   1   0\n",
      "    1   2   2   0   0   1   0]\n",
      " [  1   0   0   0   1 473   2   0   0   2   0   0   0   1   0  11   1   0\n",
      "    0   6   0   0   0   1   1]\n",
      " [  9   8   0   1   0   3 409   0   0   5   1   0   1   0   0   0  55   0\n",
      "    5   0   1   0   0   0   2]\n",
      " [  3   4   0   0   0   1   0 466   1   0   3   7   1  11   0   0   0   0\n",
      "    0   1   0   0   0   1   1]\n",
      " [  0   0   0   0   0   2   0   0 390  16   0  83   0   0   0   0   1   0\n",
      "    0   1   0   0   0   1   2]\n",
      " [  0   1   0   7   0   1   3   0   8 458   0   1   0   0   0   0   2   0\n",
      "    5   6   2   3   2   0   1]\n",
      " [  2   1   1   1   1   2   0   7   0   0 466   2   1   1   0   0   0   1\n",
      "    0   0   4   3   1   5   0]\n",
      " [  0   3   3   0   1   0   0   4 133   2   0 354   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1   0   0   0   0   0   0   3   0   0   0   0 478  13   0   0   0   1\n",
      "    0   0   2   0   2   0   0]\n",
      " [  6   1   0   3   1   0   1   5   0   0   1   0   6 462   1   0   1   1\n",
      "    0   0   0   1   6   3   0]\n",
      " [  2   0   0   5   2   0   1   0   0   1   0   0   0   0 485   0   1   0\n",
      "    1   0   2   0   0   0   0]\n",
      " [  2   0   0   3   4   3   0   0   0   0   0   0   1   2   1 476   3   1\n",
      "    0   0   0   0   1   0   3]\n",
      " [  9   1   2   1   1   1  80   0   2   0   1   1   0   1   2   3 386   1\n",
      "    0   3   1   1   1   0   0]\n",
      " [  7   0   2   0   3   4   1   2   0   0   4   0   1   3   0   6   1 456\n",
      "    0   4   0   1   0   1   3]\n",
      " [  0   1   0   0   3   0  17   0   1   3   0   0   0   1   1   0   0   0\n",
      "  472   0   0   0   0   1   0]\n",
      " [  0   1   2   0   1   8   0   1   2   5   3   2   0   0   0   0   0   2\n",
      "    0 466   0   1   0   0   6]\n",
      " [  1   0   0   1   0   0   0   2   0   0   0   0   0   0   1   0   0   1\n",
      "    0   1 477   9   2   1   4]\n",
      " [  0   0   0   1   0   0   0   0   0   1   1   0   1   3   0   0   2   6\n",
      "    0   1  22 452   0   3   7]\n",
      " [  1   0   0   0   0   0   0   1   0   0   1   0   0   5   0   0   0   0\n",
      "    0   0   7   3 480   1   1]\n",
      " [  3   0   0   0   0   1   1   0   0   0   2   0   0   4   0   0   1   2\n",
      "    0   1   0   1   0 477   6]\n",
      " [  0   1   0   1   0   0   4   2   0   3   0   0   0   0   1   0   3   4\n",
      "    0   1   4  19   0   4 453]\n",
      " [  1   0   1   1   2   1   0   0   5   0   0   2   0   0   0   1   6   1\n",
      "    0   2   0   0   1   3   0]]\n"
     ]
    }
   ],
   "source": [
    "con_mat = confusion_matrix(all_labels, all_pred)\n",
    "print('Confusion matrix: ', con_mat[1:27, 0:25])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top errors:  [[133, 'l', 'i', 11, 8], [83, 'i', 'l', 8, 11], [80, 'q', 'g', 16, 6], [55, 'g', 'q', 6, 16], [22, 'v', 'u', 21, 20], [19, 'y', 'v', 24, 21], [17, 's', 'g', 18, 6], [16, 'i', 'j', 8, 9], [14, 'd', 'o', 3, 14], [13, 'm', 'n', 12, 13], [13, 'a', 'q', 0, 16], [11, 'h', 'n', 7, 13], [11, 'f', 'p', 5, 15], [11, 'b', 'g', 1, 6]]\n"
     ]
    }
   ],
   "source": [
    "top = []\n",
    "for i in range(con_mat.shape[0]):\n",
    "    for j in range(con_mat.shape[1]):\n",
    "        if (con_mat[i, j] > 10 and i != j):\n",
    "            top.append([con_mat[i, j], classes[i], classes[j], i, j])\n",
    "top.sort(reverse=True)\n",
    "print('Top errors: ', top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l i\n",
      "i l\n",
      "q g\n",
      "g q\n",
      "v u\n",
      "y v\n",
      "s g\n",
      "i j\n",
      "d o\n",
      "m n\n"
     ]
    }
   ],
   "source": [
    "num_pics = 10    \n",
    "for i in range(len(top)):\n",
    "    if (num_pics > 0):\n",
    "        print(top[i][1], top[i][2] )\n",
    "        num_pics-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pic = plt.imread('images/train/g/g_01334.jpg') #перепутал с q\n",
    "print(pic.shape)\n",
    " \n",
    "pic = np.transpose(pic, axes=(1, 0, 2))\n",
    " \n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(pic)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(len(classes)))\n",
    "class_total = list(0. for i in range(len(classes)))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net5(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(1000):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of o = 97.0%\n",
      "Precision of w = 96.0%\n",
      "Precision of e = 95.8%\n",
      "Precision of p = 95.4%\n",
      "Precision of m = 95.2%\n",
      "Precision of x = 95.2%\n",
      "Precision of c = 95.0%\n",
      "Precision of f = 94.8%\n",
      "Precision of t = 94.8%\n",
      "Precision of u = 94.6%\n",
      "Precision of s = 94.2%\n",
      "Precision of z = 94.2%\n",
      "Precision of h = 93.8%\n",
      "Precision of k = 93.4%\n",
      "Precision of n = 93.0%\n",
      "Precision of d = 92.8%\n",
      "Precision of r = 92.6%\n",
      "Precision of b = 92.2%\n",
      "Precision of j = 91.8%\n",
      "Precision of v = 91.0%\n",
      "Precision of a = 90.2%\n",
      "Precision of y = 90.2%\n",
      "Precision of g = 81.4%\n",
      "Precision of i = 79.2%\n",
      "Precision of q = 78.4%\n",
      "Precision of l = 69.4%\n"
     ]
    }
   ],
   "source": [
    "list_with_accuracies = [(classes[i], class_correct[i] / class_total[i]) for i in range(len(classes))]\n",
    "list_with_accuracies.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    print('Precision of ', list_with_accuracies[i][0], ' = ', np.round(list_with_accuracies[i][1]*100, 1), '%', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель не идеально равномерно угадывает буквы, но чаще делает ошибки в тех, при чтении которых и люди могут допустить ошибку (l, i, g, q)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num:  3\n",
      "Top errors:  [[7, 'j', 'd', 9, 3], [6, 'b', 'd', 1, 3]]\n",
      "j d\n",
      "b d\n",
      "Precision of the surname:  97.39999999999999\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "sur_err = []\n",
    "for i in range(len(classes)):\n",
    "    if classes[i] == 'd':\n",
    "        num = i\n",
    "print('num: ', num)\n",
    "\n",
    "sur_err_sum = 0\n",
    "for i in range(con_mat.shape[1]):\n",
    "    if (con_mat[i, num] > 5 and i != num):\n",
    "        sur_err_sum += con_mat[i, num]\n",
    "        sur_err.append([con_mat[i, num], classes[i], classes[num], i, num])\n",
    "sur_err.sort(reverse=True)\n",
    "print('Top errors: ',sur_err)\n",
    "\n",
    "num_pics = 10    \n",
    "for i in range(len(sur_err)):\n",
    "    if (num_pics > 0):\n",
    "        print(sur_err[i][1], sur_err[i][2] )\n",
    "        num_pics -= 1\n",
    "    \n",
    "print('Precision of the surname: ', 100 * (1 - sur_err_sum/class_total[num]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2  \n",
    "  \n",
    "*Вес в общей оценке - 0.35*  \n",
    "  \n",
    "1. Постройте и обучите модели нейронной сети с 1-м, 2-мя и 3-мя сверточными слоями.  \n",
    "Попробуйте различные значения параметров сверток и числа фильтров на каждом слое. Оцените качество моделей с различными параметрами, проведите сравнительный анализ.  \n",
    "2. Для наилучшей конфигурации из предыдущего пункта, сравните, как меняется качество модели при увеличении размера батча при использовании BatchNorm и GroupNorm.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 сверточный слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet1(nn.Module):\n",
    "    def __init__(self,in_dim,hid,out_dim):\n",
    "        super(ConvNet1, self).__init__()\n",
    "        self.layer = nn.Sequential( \n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.BatchNorm2d(32), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(in_dim, hid)\n",
    "        self.fc2 = nn.Linear(hid, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "in_dim = 6272\n",
    "hid = 500\n",
    "out_dim = 26       \n",
    "ConvNet1 = ConvNet1(in_dim, hid, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()   \n",
    "optimizer = torch.optim.Adam(ConvNet1.parameters(), lr=0.0001, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  50\n",
      "[1,   100] loss: 0.444\n",
      "[1,   200] loss: 0.449\n",
      "[1,   300] loss: 0.430\n",
      "[1,   400] loss: 0.476\n",
      "[1,   500] loss: 0.422\n",
      "[1,   600] loss: 0.464\n",
      "[1,   700] loss: 0.451\n",
      "[1,   800] loss: 0.441\n",
      "[1,   900] loss: 0.444\n",
      "[1,  1000] loss: 0.461\n",
      "[1,  1100] loss: 0.429\n",
      "[1,  1200] loss: 0.478\n",
      "[1,  1300] loss: 0.460\n",
      "[2,   100] loss: 0.395\n",
      "[2,   200] loss: 0.412\n",
      "[2,   300] loss: 0.415\n",
      "[2,   400] loss: 0.398\n",
      "[2,   500] loss: 0.404\n",
      "[2,   600] loss: 0.422\n",
      "[2,   700] loss: 0.416\n",
      "[2,   800] loss: 0.432\n",
      "[2,   900] loss: 0.399\n",
      "[2,  1000] loss: 0.410\n",
      "[2,  1100] loss: 0.413\n",
      "[2,  1200] loss: 0.408\n",
      "[2,  1300] loss: 0.437\n",
      "Finished Training\n",
      "Accuracy: 85 %\n",
      "Batch size:  100\n",
      "[1,   100] loss: 0.363\n",
      "[1,   200] loss: 0.368\n",
      "[1,   300] loss: 0.346\n",
      "[1,   400] loss: 0.387\n",
      "[1,   500] loss: 0.366\n",
      "[1,   600] loss: 0.359\n",
      "[2,   100] loss: 0.348\n",
      "[2,   200] loss: 0.353\n",
      "[2,   300] loss: 0.342\n",
      "[2,   400] loss: 0.357\n",
      "[2,   500] loss: 0.348\n",
      "[2,   600] loss: 0.366\n",
      "Finished Training\n",
      "Accuracy: 87 %\n",
      "Batch size:  200\n",
      "[1,   100] loss: 0.314\n",
      "[1,   200] loss: 0.310\n",
      "[1,   300] loss: 0.327\n",
      "[2,   100] loss: 0.308\n",
      "[2,   200] loss: 0.311\n",
      "[2,   300] loss: 0.321\n",
      "Finished Training\n",
      "Accuracy: 88 %\n",
      "Batch size:  500\n",
      "[1,   100] loss: 0.292\n",
      "[2,   100] loss: 0.286\n",
      "Finished Training\n",
      "Accuracy: 88 %\n",
      "Batch size:  1000\n",
      "Finished Training\n",
      "Accuracy: 88 %\n"
     ]
    }
   ],
   "source": [
    "for batch in batch_size:\n",
    "    trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, batch)\n",
    "    \n",
    "    print('Batch size: ', batch)\n",
    "    \n",
    "    for epoch in range(2): \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = ConvNet1(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "     \n",
    "    print('Finished Training')\n",
    "    \n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = ConvNet1(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc1.append(100 * correct / total)\n",
    "            \n",
    "    print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(ConvNet1.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  50\n",
      "[1,   100] loss: 0.578\n",
      "[1,   200] loss: 0.676\n",
      "[1,   300] loss: 0.624\n",
      "[1,   400] loss: 0.634\n",
      "[1,   500] loss: 0.631\n",
      "[1,   600] loss: 0.581\n",
      "[1,   700] loss: 0.581\n",
      "[1,   800] loss: 0.599\n",
      "[1,   900] loss: 0.565\n",
      "[1,  1000] loss: 0.573\n",
      "[1,  1100] loss: 0.585\n",
      "[1,  1200] loss: 0.595\n",
      "[1,  1300] loss: 0.577\n",
      "[2,   100] loss: 0.508\n",
      "[2,   200] loss: 0.505\n",
      "[2,   300] loss: 0.482\n",
      "[2,   400] loss: 0.517\n",
      "[2,   500] loss: 0.522\n",
      "[2,   600] loss: 0.500\n",
      "[2,   700] loss: 0.504\n",
      "[2,   800] loss: 0.493\n",
      "[2,   900] loss: 0.497\n",
      "[2,  1000] loss: 0.545\n",
      "[2,  1100] loss: 0.492\n",
      "[2,  1200] loss: 0.502\n",
      "[2,  1300] loss: 0.473\n",
      "Finished Training\n",
      "Accuracy: 83 %\n",
      "Batch size:  100\n",
      "[1,   100] loss: 0.415\n",
      "[1,   200] loss: 0.401\n",
      "[1,   300] loss: 0.377\n",
      "[1,   400] loss: 0.416\n",
      "[1,   500] loss: 0.398\n",
      "[1,   600] loss: 0.420\n",
      "[2,   100] loss: 0.362\n",
      "[2,   200] loss: 0.377\n",
      "[2,   300] loss: 0.397\n",
      "[2,   400] loss: 0.388\n",
      "[2,   500] loss: 0.380\n",
      "[2,   600] loss: 0.394\n",
      "Finished Training\n",
      "Accuracy: 86 %\n",
      "Batch size:  200\n",
      "[1,   100] loss: 0.309\n",
      "[1,   200] loss: 0.340\n",
      "[1,   300] loss: 0.349\n",
      "[2,   100] loss: 0.327\n",
      "[2,   200] loss: 0.314\n",
      "[2,   300] loss: 0.330\n",
      "Finished Training\n",
      "Accuracy: 87 %\n",
      "Batch size:  500\n",
      "[1,   100] loss: 0.282\n",
      "[2,   100] loss: 0.274\n",
      "Finished Training\n",
      "Accuracy: 88 %\n",
      "Batch size:  1000\n",
      "Finished Training\n",
      "Accuracy: 89 %\n"
     ]
    }
   ],
   "source": [
    "for batch in batch_size:\n",
    "    trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, batch)\n",
    "    \n",
    "    print('Batch size: ', batch)\n",
    "    \n",
    "    for epoch in range(2): \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = ConvNet1(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "     \n",
    "    print('Finished Training')\n",
    "    \n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = ConvNet1(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc1_.append(100 * correct / total)\n",
    "            \n",
    "    print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат у модели при learning_rate = 0.0005 и batch_size = 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(ConvNet1.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet1_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet1_1, self).__init__()\n",
    "        self.layer1 = nn.Sequential( nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "        nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(6272, 500)\n",
    "        self.fc2 = nn.Linear(500, 26)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "        \n",
    "ConvNet1_1 = ConvNet1_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(ConvNet1_1.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n",
      "Accuracy: 83 %\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ConvNet1_1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "     \n",
    "print('Finished Training')\n",
    "    \n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = ConvNet1_1(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "            \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 сверточных слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet2(nn.Module):\n",
    "    def __init__(self,in_dim,hid,out_dim):\n",
    "        super(ConvNet2, self).__init__()\n",
    "        self.layer1 = nn.Sequential( \n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(in_dim, hid)\n",
    "        self.fc2 = nn.Linear(hid, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "in_dim = 3136\n",
    "hid = 500\n",
    "out_dim = 26\n",
    "ConvNet2 = ConvNet2(in_dim, hid, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()      \n",
    "optimizer = torch.optim.Adam(ConvNet2.parameters(), lr=0.0001, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc2_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  50\n",
      "[1,   100] loss: 1.971\n",
      "[1,   200] loss: 1.069\n",
      "[1,   300] loss: 0.851\n",
      "[1,   400] loss: 0.708\n",
      "[1,   500] loss: 0.612\n",
      "[1,   600] loss: 0.561\n",
      "[1,   700] loss: 0.531\n",
      "[1,   800] loss: 0.527\n",
      "[1,   900] loss: 0.504\n",
      "[1,  1000] loss: 0.482\n",
      "[1,  1100] loss: 0.459\n",
      "[1,  1200] loss: 0.452\n",
      "[1,  1300] loss: 0.430\n",
      "[2,   100] loss: 0.376\n",
      "[2,   200] loss: 0.407\n",
      "[2,   300] loss: 0.378\n",
      "[2,   400] loss: 0.368\n",
      "[2,   500] loss: 0.373\n",
      "[2,   600] loss: 0.375\n",
      "[2,   700] loss: 0.352\n",
      "[2,   800] loss: 0.358\n",
      "[2,   900] loss: 0.355\n",
      "[2,  1000] loss: 0.325\n",
      "[2,  1100] loss: 0.342\n",
      "[2,  1200] loss: 0.353\n",
      "[2,  1300] loss: 0.325\n",
      "Finished Training\n",
      "Accuracy: 89 %\n",
      "Batch size:  100\n",
      "[1,   100] loss: 0.295\n",
      "[1,   200] loss: 0.294\n",
      "[1,   300] loss: 0.293\n",
      "[1,   400] loss: 0.291\n",
      "[1,   500] loss: 0.285\n",
      "[1,   600] loss: 0.295\n",
      "[2,   100] loss: 0.273\n",
      "[2,   200] loss: 0.273\n",
      "[2,   300] loss: 0.257\n",
      "[2,   400] loss: 0.264\n",
      "[2,   500] loss: 0.253\n",
      "[2,   600] loss: 0.263\n",
      "Finished Training\n",
      "Accuracy: 91 %\n",
      "Batch size:  200\n",
      "[1,   100] loss: 0.237\n",
      "[1,   200] loss: 0.236\n",
      "[1,   300] loss: 0.234\n",
      "[2,   100] loss: 0.224\n",
      "[2,   200] loss: 0.232\n",
      "[2,   300] loss: 0.222\n",
      "Finished Training\n",
      "Accuracy: 91 %\n",
      "Batch size:  500\n",
      "[1,   100] loss: 0.209\n",
      "[2,   100] loss: 0.204\n",
      "Finished Training\n",
      "Accuracy: 92 %\n",
      "Batch size:  1000\n",
      "Finished Training\n",
      "Accuracy: 92 %\n"
     ]
    }
   ],
   "source": [
    "for batch in batch_size:\n",
    "    trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, batch)\n",
    "    \n",
    "    print('Batch size: ', batch)\n",
    "    \n",
    "    for epoch in range(2): \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = ConvNet2(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "     \n",
    "    print('Finished Training')\n",
    "    \n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = ConvNet2(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc2_.append(100 * correct / total)\n",
    "            \n",
    "    print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()      \n",
    "optimizer = torch.optim.Adam(ConvNet2.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  50\n",
      "[1,   100] loss: 0.322\n",
      "[1,   200] loss: 0.350\n",
      "[1,   300] loss: 0.365\n",
      "[1,   400] loss: 0.356\n",
      "[1,   500] loss: 0.345\n",
      "[1,   600] loss: 0.386\n",
      "[1,   700] loss: 0.360\n",
      "[1,   800] loss: 0.343\n",
      "[1,   900] loss: 0.371\n",
      "[1,  1000] loss: 0.345\n",
      "[1,  1100] loss: 0.347\n",
      "[1,  1200] loss: 0.378\n",
      "[1,  1300] loss: 0.340\n",
      "[2,   100] loss: 0.299\n",
      "[2,   200] loss: 0.330\n",
      "[2,   300] loss: 0.326\n",
      "[2,   400] loss: 0.311\n",
      "[2,   500] loss: 0.330\n",
      "[2,   600] loss: 0.341\n",
      "[2,   700] loss: 0.333\n",
      "[2,   800] loss: 0.312\n",
      "[2,   900] loss: 0.308\n",
      "[2,  1000] loss: 0.307\n",
      "[2,  1100] loss: 0.335\n",
      "[2,  1200] loss: 0.327\n",
      "[2,  1300] loss: 0.311\n",
      "Finished Training\n",
      "Accuracy: 89 %\n",
      "Batch size:  100\n",
      "[1,   100] loss: 0.242\n",
      "[1,   200] loss: 0.254\n",
      "[1,   300] loss: 0.248\n",
      "[1,   400] loss: 0.248\n",
      "[1,   500] loss: 0.264\n",
      "[1,   600] loss: 0.272\n",
      "[2,   100] loss: 0.221\n",
      "[2,   200] loss: 0.226\n",
      "[2,   300] loss: 0.234\n",
      "[2,   400] loss: 0.252\n",
      "[2,   500] loss: 0.249\n",
      "[2,   600] loss: 0.244\n",
      "Finished Training\n",
      "Accuracy: 91 %\n",
      "Batch size:  200\n",
      "[1,   100] loss: 0.196\n",
      "[1,   200] loss: 0.198\n",
      "[1,   300] loss: 0.197\n",
      "[2,   100] loss: 0.186\n",
      "[2,   200] loss: 0.187\n",
      "[2,   300] loss: 0.199\n",
      "Finished Training\n",
      "Accuracy: 92 %\n",
      "Batch size:  500\n",
      "[1,   100] loss: 0.159\n",
      "[2,   100] loss: 0.150\n",
      "Finished Training\n",
      "Accuracy: 93 %\n",
      "Batch size:  1000\n",
      "Finished Training\n",
      "Accuracy: 93 %\n"
     ]
    }
   ],
   "source": [
    "for batch in batch_size:\n",
    "    trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, batch)\n",
    "    \n",
    "    print('Batch size: ', batch)\n",
    "    \n",
    "    for epoch in range(2): \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = ConvNet2(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "     \n",
    "    print('Finished Training')\n",
    "    \n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = ConvNet2(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc2.append(100 * correct / total)\n",
    "            \n",
    "    print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet2_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet2_1, self).__init__()\n",
    "        self.layer1 = nn.Sequential( \n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential( \n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(3136, 500)\n",
    "        self.fc2 = nn.Linear(500, 26)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "ConvNet2_1 = ConvNet2_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(ConvNet2_1.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.344\n",
      "[2,   100] loss: 0.297\n",
      "Finished Training\n",
      "Accuracy: 90 %\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ConvNet2_1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "     \n",
    "print('Finished Training')\n",
    "    \n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = ConvNet2_1(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "            \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 сверточных слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet3(nn.Module):\n",
    "    def __init__(self,in_dim,hid,out_dim):\n",
    "        super(ConvNet3, self).__init__()\n",
    "        self.layer1 = nn.Sequential( \n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential( \n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential( \n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(in_dim, hid)\n",
    "        self.fc2 = nn.Linear(hid, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "in_dim = 1152\n",
    "hid = 500\n",
    "out_dim = 26    \n",
    "ConvNet3 = ConvNet3(in_dim, hid, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()      \n",
    "optimizer = torch.optim.Adam(ConvNet3.parameters(), lr=0.0001, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  50\n",
      "[1,   100] loss: 2.941\n",
      "[1,   200] loss: 1.646\n",
      "[1,   300] loss: 1.140\n",
      "[1,   400] loss: 0.933\n",
      "[1,   500] loss: 0.810\n",
      "[1,   600] loss: 0.700\n",
      "[1,   700] loss: 0.627\n",
      "[1,   800] loss: 0.594\n",
      "[1,   900] loss: 0.534\n",
      "[1,  1000] loss: 0.529\n",
      "[1,  1100] loss: 0.495\n",
      "[1,  1200] loss: 0.467\n",
      "[1,  1300] loss: 0.448\n",
      "[2,   100] loss: 0.427\n",
      "[2,   200] loss: 0.419\n",
      "[2,   300] loss: 0.415\n",
      "[2,   400] loss: 0.397\n",
      "[2,   500] loss: 0.386\n",
      "[2,   600] loss: 0.376\n",
      "[2,   700] loss: 0.372\n",
      "[2,   800] loss: 0.360\n",
      "[2,   900] loss: 0.360\n",
      "[2,  1000] loss: 0.346\n",
      "[2,  1100] loss: 0.324\n",
      "[2,  1200] loss: 0.334\n",
      "[2,  1300] loss: 0.329\n",
      "Finished Training\n",
      "Accuracy: 89 %\n",
      "Batch size:  100\n",
      "[1,   100] loss: 0.304\n",
      "[1,   200] loss: 0.307\n",
      "[1,   300] loss: 0.294\n",
      "[1,   400] loss: 0.279\n",
      "[1,   500] loss: 0.284\n",
      "[1,   600] loss: 0.283\n",
      "[2,   100] loss: 0.260\n",
      "[2,   200] loss: 0.267\n",
      "[2,   300] loss: 0.270\n",
      "[2,   400] loss: 0.245\n",
      "[2,   500] loss: 0.257\n",
      "[2,   600] loss: 0.252\n",
      "Finished Training\n",
      "Accuracy: 91 %\n",
      "Batch size:  200\n",
      "[1,   100] loss: 0.227\n",
      "[1,   200] loss: 0.228\n",
      "[1,   300] loss: 0.240\n",
      "[2,   100] loss: 0.222\n",
      "[2,   200] loss: 0.220\n",
      "[2,   300] loss: 0.219\n",
      "Finished Training\n",
      "Accuracy: 91 %\n",
      "Batch size:  500\n",
      "[1,   100] loss: 0.203\n",
      "[2,   100] loss: 0.197\n",
      "Finished Training\n",
      "Accuracy: 92 %\n",
      "Batch size:  1000\n",
      "Finished Training\n",
      "Accuracy: 92 %\n"
     ]
    }
   ],
   "source": [
    "for batch in batch_size:\n",
    "    trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, batch)\n",
    "    \n",
    "    print('Batch size: ', batch)\n",
    "    \n",
    "    for epoch in range(2): \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = ConvNet3(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "     \n",
    "    print('Finished Training')\n",
    "    \n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = ConvNet3(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc3.append(100 * correct / total)\n",
    "            \n",
    "    print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(ConvNet3.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc3_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  50\n",
      "[1,   100] loss: 0.361\n",
      "[1,   200] loss: 0.359\n",
      "[1,   300] loss: 0.322\n",
      "[1,   400] loss: 0.322\n",
      "[1,   500] loss: 0.321\n",
      "[1,   600] loss: 0.327\n",
      "[1,   700] loss: 0.312\n",
      "[1,   800] loss: 0.311\n",
      "[1,   900] loss: 0.297\n",
      "[1,  1000] loss: 0.299\n",
      "[1,  1100] loss: 0.337\n",
      "[1,  1200] loss: 0.259\n",
      "[1,  1300] loss: 0.292\n",
      "[2,   100] loss: 0.262\n",
      "[2,   200] loss: 0.258\n",
      "[2,   300] loss: 0.260\n",
      "[2,   400] loss: 0.268\n",
      "[2,   500] loss: 0.245\n",
      "[2,   600] loss: 0.259\n",
      "[2,   700] loss: 0.279\n",
      "[2,   800] loss: 0.256\n",
      "[2,   900] loss: 0.258\n",
      "[2,  1000] loss: 0.260\n",
      "[2,  1100] loss: 0.236\n",
      "[2,  1200] loss: 0.234\n",
      "[2,  1300] loss: 0.251\n",
      "Finished Training\n",
      "Accuracy: 91 %\n",
      "Batch size:  100\n",
      "[1,   100] loss: 0.206\n",
      "[1,   200] loss: 0.182\n",
      "[1,   300] loss: 0.198\n",
      "[1,   400] loss: 0.204\n",
      "[1,   500] loss: 0.198\n",
      "[1,   600] loss: 0.199\n",
      "[2,   100] loss: 0.178\n",
      "[2,   200] loss: 0.179\n",
      "[2,   300] loss: 0.188\n",
      "[2,   400] loss: 0.192\n",
      "[2,   500] loss: 0.183\n",
      "[2,   600] loss: 0.191\n",
      "Finished Training\n",
      "Accuracy: 92 %\n",
      "Batch size:  200\n",
      "[1,   100] loss: 0.154\n",
      "[1,   200] loss: 0.149\n",
      "[1,   300] loss: 0.153\n",
      "[2,   100] loss: 0.136\n",
      "[2,   200] loss: 0.141\n",
      "[2,   300] loss: 0.150\n",
      "Finished Training\n",
      "Accuracy: 93 %\n",
      "Batch size:  500\n",
      "[1,   100] loss: 0.114\n",
      "[2,   100] loss: 0.108\n",
      "Finished Training\n",
      "Accuracy: 93 %\n",
      "Batch size:  1000\n",
      "Finished Training\n",
      "Accuracy: 93 %\n"
     ]
    }
   ],
   "source": [
    "for batch in batch_size:\n",
    "    trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, batch)\n",
    "    \n",
    "    print('Batch size: ', batch)\n",
    "    \n",
    "    for epoch in range(2): \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = ConvNet3(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "     \n",
    "    print('Finished Training')\n",
    "    \n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = ConvNet3(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc3.append(100 * correct / total)\n",
    "            \n",
    "    print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут можно взять batch_size = 500 и learning_rate = 0.0005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet3_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet3_1, self).__init__()\n",
    "        self.layer1 = nn.Sequential( \n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential( \n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential( \n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(1152, 500)\n",
    "        self.fc2 = nn.Linear(500, 26)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "ConvNet3_1 = ConvNet3_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(ConvNet3_1.parameters(), lr=0.0005, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.263\n",
      "[2,   100] loss: 0.408\n",
      "Finished Training\n",
      "Accuracy: 88 %\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ConvNet3_1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "     \n",
    "print('Finished Training')\n",
    "    \n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = ConvNet3_1(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "            \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNorm and GroupNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем делать для модели с 2мя сверточными слоями, так как ее accuracy на подобранных параметрах не отличается от accuracy для модели с 3мя сверточными слоями, а времени займет меньше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet2_bn(nn.Module):\n",
    "    def __init__(self,in_dim,hid,out_dim):\n",
    "        super(ConvNet2_bn, self).__init__()\n",
    "        self.layer1 = nn.Sequential( \n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(start_dim=1), \n",
    "            nn.Linear(in_features=3136, out_features=500), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.Linear(in_features=500, out_features=200),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(in_dim, hid)\n",
    "        self.fc2 = nn.Linear(hid, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "in_dim = 200\n",
    "hid = 100\n",
    "out_dim = 26\n",
    "ConvNet2_bn = ConvNet2_bn(in_dim, hid, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()      #пробуем для лучших параметров модели с 2мя слоями\n",
    "optimizer = torch.optim.Adam(ConvNet2_bn.parameters(), lr=0.0005, betas=(0.9, 0.99))\n",
    "trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.199\n",
      "[2,   100] loss: 0.158\n",
      "Finished Training\n",
      "Accuracy: 92 %\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ConvNet2_bn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "     \n",
    "print('Finished Training')\n",
    "    \n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = ConvNet2_bn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "            \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже код для подбора batch_size для ConvNet2_bn, но он работает очень долго. В любом случае лучшими параметрами (из тех, на которых мы подбираем для каждой модели) окажутся те, на которых я запускаю эту модель в верхней ячейке. Для ConvNet2_gn лучшие параметры будут аналогичны, код там на подборе тоже долго работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch in batch_size:\n",
    "#    trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, batch)\n",
    "    \n",
    "#    print('Batch size: ', batch)\n",
    "    \n",
    "#    for epoch in range(2): \n",
    "\n",
    "#        running_loss = 0.0\n",
    "#        for i, data in enumerate(trainloader, 0):\n",
    "#            inputs, labels = data\n",
    "\n",
    "#            optimizer.zero_grad()\n",
    "\n",
    "#            outputs = ConvNet2_bn(inputs)\n",
    "#            loss = criterion(outputs, labels)\n",
    "#            loss.backward()\n",
    "#            optimizer.step()\n",
    "\n",
    "#            running_loss += loss.item()\n",
    "#            if i % 100 == 99:\n",
    "#                print('[%d, %5d] loss: %.3f' %\n",
    "#                      (epoch + 1, i + 1, running_loss / 100))\n",
    "#                running_loss = 0.0\n",
    "     \n",
    "#    print('Finished Training')\n",
    "    \n",
    "#    dataiter = iter(testloader)\n",
    "#    images, labels = dataiter.next()\n",
    "    \n",
    "#    correct = 0\n",
    "#    total = 0\n",
    "#    with torch.no_grad():\n",
    "#        for data in testloader:\n",
    "#            images, labels = data\n",
    "#            outputs = ConvNet2_bn(images)\n",
    "#            _, predicted = torch.max(outputs.data, 1)\n",
    "#            total += labels.size(0)\n",
    "#            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "#    print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet2_gn(nn.Module):\n",
    "    def __init__(self,in_dim,hid,out_dim):\n",
    "        super(ConvNet2_gn, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.GroupNorm(3, 3),\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.GroupNorm(32, 32),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(start_dim=1), \n",
    "            nn.Linear(in_features=3136, out_features=500), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.Linear(in_features=500, out_features=200),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(in_dim, hid)\n",
    "        self.fc2 = nn.Linear(hid, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "in_dim = 200\n",
    "hid = 100\n",
    "out_dim = 26\n",
    "ConvNet2_gn = ConvNet2_gn(in_dim, hid, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()      #пробуем для лучших параметров модели с 2мя слоями\n",
    "optimizer = torch.optim.Adam(ConvNet2_gn.parameters(), lr=0.0005, betas=(0.9, 0.99))\n",
    "trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.956\n",
      "[2,   100] loss: 0.272\n",
      "Finished Training\n",
      "Accuracy: 91 %\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ConvNet2_gn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "     \n",
    "print('Finished Training')\n",
    "    \n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = ConvNet2_gn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "            \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подбор для ConvNet2_gn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch in batch_size:\n",
    "#    trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, batch)\n",
    "    \n",
    "#    print('Batch size: ', batch)\n",
    "    \n",
    "#    for epoch in range(2): \n",
    "\n",
    "#        running_loss = 0.0\n",
    "#        for i, data in enumerate(trainloader, 0):\n",
    "#            inputs, labels = data\n",
    "\n",
    "#            optimizer.zero_grad()\n",
    "\n",
    "#            outputs = ConvNet2_gn(inputs)\n",
    "#            loss = criterion(outputs, labels)\n",
    "#            loss.backward()\n",
    "#            optimizer.step()\n",
    "\n",
    "#            running_loss += loss.item()\n",
    "#            if i % 100 == 99:\n",
    "#                print('[%d, %5d] loss: %.3f' %\n",
    "#                      (epoch + 1, i + 1, running_loss / 100))\n",
    "#                running_loss = 0.0\n",
    "     \n",
    "#    print('Finished Training')\n",
    "    \n",
    "#    dataiter = iter(testloader)\n",
    "#    images, labels = dataiter.next()\n",
    "    \n",
    "#    correct = 0\n",
    "#    total = 0\n",
    "#    with torch.no_grad():\n",
    "#        for data in testloader:\n",
    "#            images, labels = data\n",
    "#            outputs = ConvNet2_gn(images)\n",
    "#            _, predicted = torch.max(outputs.data, 1)\n",
    "#            total += labels.size(0)\n",
    "#            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "#    print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3    \n",
    "  \n",
    "Обучите модель с точностью (accuracy) на тестовых данных:  \n",
    "- `>= 0.85`    +1 балл\n",
    "- `>= 0.95`    +2 балла\n",
    "- `>= 0.99`    +3 балла  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet2_bn1(nn.Module):\n",
    "    def __init__(self,in_dim,hid,out_dim):\n",
    "        super(ConvNet2_bn1, self).__init__()\n",
    "        self.layer1 = nn.Sequential( \n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(start_dim=1), \n",
    "            nn.Linear(in_features=3136, out_features=500), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.Linear(in_features=500, out_features=200),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(in_dim, hid)\n",
    "        self.fc2 = nn.Linear(hid, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer1(x)\n",
    "        x1 = x1.reshape(x1.size(0), -1)\n",
    "        x1 = self.dropout(x1)\n",
    "        x1 = self.fc1(x1)\n",
    "        x1 = self.fc2(x1)\n",
    "        return x1\n",
    "    \n",
    "in_dim = 200\n",
    "hid = 100\n",
    "out_dim = 26\n",
    "ConvNet2_bn1 = ConvNet2_bn1(in_dim, hid, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()   \n",
    "optimizer = torch.optim.Adam(ConvNet2_bn1.parameters(), lr=0.0005, betas=(0.9, 0.99))\n",
    "trainloader, testloader, classes, dataset_sizes=get_dataset('images',data_transforms, 310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.169\n",
      "[1,   200] loss: 0.166\n",
      "[2,   100] loss: 0.136\n",
      "[2,   200] loss: 0.146\n",
      "[3,   100] loss: 0.121\n",
      "[3,   200] loss: 0.125\n",
      "[4,   100] loss: 0.099\n",
      "[4,   200] loss: 0.114\n",
      "[5,   100] loss: 0.089\n",
      "[5,   200] loss: 0.105\n",
      "[6,   100] loss: 0.078\n",
      "[6,   200] loss: 0.086\n",
      "Finished Training\n",
      "Accuracy: 92 %\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(6): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ConvNet2_bn1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "     \n",
    "print('Finished Training')\n",
    "    \n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = ConvNet2_bn1(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "            \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусные задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1 (1 балл).**  \n",
    "\n",
    "Напишите на листке белой бумаги (маркером или ручкой) от 5 разных букв (можно больше 5 букв в целом с повторениями, но должно быть минимум 5 разных) английского алфавита (в датасете есть как прописные, так и строчные буквы). Сфотографируйте букву и приведите её картинку к размеру $28\\times28$ и, желательно, к чёрно-белой палитре цветов. Передайте получившиеся изображения вашей модели и выполните предсказание, оцените результат.  \n",
    "  \n",
    "**Tips:**  \n",
    "- В датасете все буквы занимают практически всё пространство картинки по высоте или ширине (или вместе). Если ваша буква будет слишком маленькой или большой, это может повлиять на результат детекции.\n",
    "- Помните, что буква должна быть белого цвета, а фон - чёрного.\n",
    "- Описание ваших действий при выполнении этого задания (что вы использовали, чтобы привести картинку к нужному виду) категорически приветствуется :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2 (1 балл):**    \n",
    "  \n",
    "Используйте transfer learning подход для решения задачи - дообучите какую-либо модель, предобученную на ImageNet, для классификации рукописных букв. Оцените качество решения.  \n",
    "В качестве предобученой модели можно взять одну из [torchvision models](https://pytorch.org/docs/stable/torchvision/models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3 (1 балл):**  \n",
    "  \n",
    "Добавьте вывод значений функции потерь и accuracy в tensorboard.  \n",
    "Метрики нужно выводить и для обучающей, и для тестовой выборки."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
